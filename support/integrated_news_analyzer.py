#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ
- ë¬´ë£Œ ë‰´ìŠ¤ í¬ë¡¤ë§ + KoBERT ê°ì„±ë¶„ì„ + Alpha Vantage ë°ì´í„° í†µí•©
- GPT-5 ê±°ë˜ ê²°ì •ì„ ìœ„í•œ ì‹œì¥ ê°ì„± ë° ë‰´ìŠ¤ ë¶„ì„ ì œê³µ
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import pandas as pd

# ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
from .free_news_crawler import FreeKoreanNewsCrawler, NewsDataManager
from .kobert_sentiment_analyzer import NewssentimentProcessor
from .alpha_vantage_connector import FreeDataAggregator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntegratedNewsAnalyzer:
    """í†µí•© ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, alpha_vantage_key: str = "demo", cache_dir: str = "news_analysis_cache"):
        """
        í†µí•© ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            alpha_vantage_key: Alpha Vantage API í‚¤ (ë¬´ë£Œ ê³„ì •)
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.news_manager = NewsDataManager(str(self.cache_dir / "korean_news_cache.json"))
        self.sentiment_processor = NewssentimentProcessor()
        self.data_aggregator = FreeDataAggregator(alpha_vantage_key)
        
        # ë¶„ì„ ì„¤ì •
        self.update_interval_hours = 1  # 1ì‹œê°„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
        self.max_news_age_hours = 24    # 24ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë§Œ ì‚¬ìš©
        
        logger.info("í†µí•© ë‰´ìŠ¤ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def update_all_data(self) -> Dict[str, Any]:
        """ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ì—…ë°ì´íŠ¸"""
        logger.info("=== í†µí•© ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘ ===")
        
        update_results = {
            'timestamp': datetime.now().isoformat(),
            'korean_news': {},
            'market_data': {},
            'sentiment_analysis': {},
            'errors': []
        }
        
        try:
            # 1. í•œêµ­ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸
            logger.info("1. í•œêµ­ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
            korean_articles = await self.news_manager.update_news_cache()
            update_results['korean_news'] = {
                'article_count': len(korean_articles),
                'sources': list(set(article.source for article in korean_articles)),
                'latest_article': korean_articles[0].published_date.isoformat() if korean_articles else None
            }
            logger.info(f"í•œêµ­ ë‰´ìŠ¤ {len(korean_articles)}ê°œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            error_msg = f"í•œêµ­ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            update_results['errors'].append(error_msg)
            korean_articles = []
        
        try:
            # 2. ì‹œì¥ ë°ì´í„° ì—…ë°ì´íŠ¸
            logger.info("2. Alpha Vantage ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘")
            market_data = self.data_aggregator.get_comprehensive_market_data()
            update_results['market_data'] = {
                'market_sentiment': market_data['market_summary']['market_sentiment'],
                'stocks_analyzed': len(market_data['market_summary']['stocks']),
                'international_news': len(market_data['market_news']),
                'api_requests_remaining': market_data['api_status']['requests_remaining']
            }
            logger.info("ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            error_msg = f"ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            update_results['errors'].append(error_msg)
            market_data = {'market_summary': {}, 'market_news': []}
        
        try:
            # 3. ê°ì„±ë¶„ì„ ìˆ˜í–‰
            if korean_articles:
                logger.info("3. KoBERT ê°ì„±ë¶„ì„ ìˆ˜í–‰")
                processed_articles = self.sentiment_processor.process_news_articles(korean_articles)
                sentiment_summary = self.sentiment_processor.get_market_sentiment_summary(processed_articles)
                
                update_results['sentiment_analysis'] = {
                    'overall_sentiment': sentiment_summary['overall_sentiment'],
                    'sentiment_score': sentiment_summary['sentiment_score'],
                    'confidence': sentiment_summary['confidence'],
                    'positive_articles': sentiment_summary['sentiment_distribution']['positive'],
                    'negative_articles': sentiment_summary['sentiment_distribution']['negative'],
                    'neutral_articles': sentiment_summary['sentiment_distribution']['neutral']
                }
                logger.info("ê°ì„±ë¶„ì„ ì™„ë£Œ")
            else:
                processed_articles = []
                sentiment_summary = {}
                
        except Exception as e:
            error_msg = f"ê°ì„±ë¶„ì„ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            update_results['errors'].append(error_msg)
            processed_articles = []
            sentiment_summary = {}
        
        # 4. í†µí•© ë¶„ì„ ë°ì´í„° ì €ì¥
        try:
            integrated_data = {
                'update_time': datetime.now().isoformat(),
                'korean_articles': processed_articles,
                'market_data': market_data,
                'sentiment_summary': sentiment_summary,
                'update_results': update_results
            }
            
            # JSONìœ¼ë¡œ ì €ì¥
            with open(self.cache_dir / "integrated_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(integrated_data, f, ensure_ascii=False, indent=2)
            
            logger.info("í†µí•© ë¶„ì„ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            error_msg = f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            update_results['errors'].append(error_msg)
        
        logger.info("=== í†µí•© ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ ===")
        return update_results
    
    def get_trading_decision_data(self, symbol: str = None) -> Dict[str, Any]:
        """
        ê±°ë˜ ê²°ì •ì„ ìœ„í•œ ë¶„ì„ ë°ì´í„° ì œê³µ
        GPT-5 ê±°ë˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ê°€ê³µ
        """
        try:
            # ì €ì¥ëœ í†µí•© ë°ì´í„° ë¡œë“œ
            with open(self.cache_dir / "integrated_analysis.json", 'r', encoding='utf-8') as f:
                integrated_data = json.load(f)
        except FileNotFoundError:
            logger.warning("í†µí•© ë°ì´í„° ì—†ìŒ - ìƒˆë¡œ ìˆ˜ì§‘ í•„ìš”")
            return {'error': 'ë°ì´í„° ì—†ìŒ', 'recommendation': 'ë°ì´í„° ì—…ë°ì´íŠ¸ í•„ìš”'}
        
        # ë°ì´í„° ì‹ ì„ ë„ ì²´í¬
        update_time = datetime.fromisoformat(integrated_data['update_time'])
        hours_old = (datetime.now() - update_time).total_seconds() / 3600
        
        if hours_old > self.update_interval_hours:
            logger.warning(f"ë°ì´í„°ê°€ {hours_old:.1f}ì‹œê°„ ì˜¤ë˜ë¨ - ì—…ë°ì´íŠ¸ ê¶Œì¥")
        
        # ê±°ë˜ ê²°ì •ìš© ë°ì´í„° êµ¬ì„±
        decision_data = {
            'timestamp': integrated_data['update_time'],
            'data_freshness_hours': hours_old,
            'market_sentiment': self._extract_market_sentiment(integrated_data),
            'news_analysis': self._extract_news_analysis(integrated_data, symbol),
            'technical_data': self._extract_technical_data(integrated_data, symbol),
            'risk_factors': self._extract_risk_factors(integrated_data),
            'confidence_score': self._calculate_confidence_score(integrated_data)
        }
        
        return decision_data
    
    def _extract_market_sentiment(self, data: Dict) -> Dict[str, Any]:
        """ì‹œì¥ ê°ì„± ì¶”ì¶œ"""
        sentiment_summary = data.get('sentiment_summary', {})
        market_data = data.get('market_data', {}).get('market_summary', {})
        
        return {
            'korean_news_sentiment': sentiment_summary.get('overall_sentiment', 'neutral'),
            'korean_sentiment_score': sentiment_summary.get('sentiment_score', 0.0),
            'korean_sentiment_confidence': sentiment_summary.get('confidence', 0.5),
            'international_sentiment': market_data.get('market_sentiment', 'neutral'),
            'major_stocks_change': market_data.get('average_change_percent', 0.0),
            'sentiment_distribution': sentiment_summary.get('sentiment_distribution', {}),
        }
    
    def _extract_news_analysis(self, data: Dict, symbol: str = None) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë¶„ì„ ì¶”ì¶œ"""
        korean_articles = data.get('korean_articles', [])
        international_news = data.get('market_data', {}).get('market_news', [])
        
        # ì‹¬ë³¼ë³„ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§
        relevant_korean = []
        if symbol and korean_articles:
            symbol_keywords = {
                '005930': ['ì‚¼ì„±ì „ì', 'ì‚¼ì„±', 'ë°˜ë„ì²´', 'ë©”ëª¨ë¦¬'],
                '000660': ['SKí•˜ì´ë‹‰ìŠ¤', 'SK', 'í•˜ì´ë‹‰ìŠ¤', 'ë°˜ë„ì²´'],
                '035420': ['ë„¤ì´ë²„', 'NAVER', 'ì¸í„°ë„·', 'í”Œë«í¼'],
                '051910': ['LGí™”í•™', 'LG', 'í™”í•™', 'ë°°í„°ë¦¬'],
                '005380': ['í˜„ëŒ€ì°¨', 'í˜„ëŒ€', 'ìë™ì°¨', 'ì „ê¸°ì°¨']
            }
            
            keywords = symbol_keywords.get(symbol, [])
            for article in korean_articles:
                if any(keyword in article.get('title', '') or keyword in str(article.get('keywords', [])) for keyword in keywords):
                    relevant_korean.append(article)
        
        return {
            'korean_news_count': len(korean_articles),
            'relevant_korean_news': relevant_korean[:5],  # ìƒìœ„ 5ê°œ
            'international_news_count': len(international_news),
            'top_international_news': international_news[:3],  # ìƒìœ„ 3ê°œ
            'key_topics': self._extract_key_topics(korean_articles + international_news)
        }
    
    def _extract_technical_data(self, data: Dict, symbol: str = None) -> Dict[str, Any]:
        """ê¸°ìˆ ì  ë°ì´í„° ì¶”ì¶œ"""
        market_summary = data.get('market_data', {}).get('market_summary', {})
        stocks_data = market_summary.get('stocks', {})
        
        if symbol and symbol in stocks_data:
            stock_data = stocks_data[symbol]
            return {
                'current_price': stock_data.get('price'),
                'price_change': stock_data.get('change'),
                'price_change_percent': stock_data.get('change_percent'),
                'volume': stock_data.get('volume'),
                'relative_performance': self._calculate_relative_performance(stock_data, market_summary)
            }
        else:
            return {
                'market_average_change': market_summary.get('average_change_percent', 0.0),
                'market_sentiment': market_summary.get('market_sentiment', 'neutral'),
                'data_available': False
            }
    
    def _extract_risk_factors(self, data: Dict) -> List[Dict[str, Any]]:
        """ë¦¬ìŠ¤í¬ ìš”ì¸ ì¶”ì¶œ"""
        risk_factors = []
        
        # ë°ì´í„° ì‹ ì„ ë„ ë¦¬ìŠ¤í¬
        update_time = datetime.fromisoformat(data['update_time'])
        hours_old = (datetime.now() - update_time).total_seconds() / 3600
        if hours_old > 2:
            risk_factors.append({
                'type': 'data_freshness',
                'severity': 'medium' if hours_old < 6 else 'high',
                'description': f'ë°ì´í„°ê°€ {hours_old:.1f}ì‹œê°„ ì˜¤ë˜ë¨'
            })
        
        # API í•œë„ ë¦¬ìŠ¤í¬
        api_status = data.get('market_data', {}).get('api_status', {})
        remaining_requests = api_status.get('requests_remaining', 500)
        if remaining_requests < 50:
            risk_factors.append({
                'type': 'api_limit',
                'severity': 'high' if remaining_requests < 20 else 'medium',
                'description': f'Alpha Vantage API ì”ì—¬ ìš”ì²­: {remaining_requests}ê°œ'
            })
        
        # ê°ì„± ì‹ ë¢°ë„ ë¦¬ìŠ¤í¬
        sentiment_confidence = data.get('sentiment_summary', {}).get('confidence', 0.5)
        if sentiment_confidence < 0.6:
            risk_factors.append({
                'type': 'sentiment_confidence',
                'severity': 'medium',
                'description': f'ê°ì„±ë¶„ì„ ì‹ ë¢°ë„ ë‚®ìŒ: {sentiment_confidence:.2f}'
            })
        
        return risk_factors
    
    def _calculate_confidence_score(self, data: Dict) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        scores = []
        
        # ë°ì´í„° ì‹ ì„ ë„ (0-1)
        update_time = datetime.fromisoformat(data['update_time'])
        hours_old = (datetime.now() - update_time).total_seconds() / 3600
        freshness_score = max(0, 1 - (hours_old / 24))  # 24ì‹œê°„ ê¸°ì¤€
        scores.append(freshness_score * 0.3)
        
        # ë‰´ìŠ¤ ë°ì´í„° í’ˆì§ˆ (0-1)
        korean_count = len(data.get('korean_articles', []))
        news_score = min(1.0, korean_count / 20)  # 20ê°œ ê¸°ì¤€
        scores.append(news_score * 0.3)
        
        # ê°ì„±ë¶„ì„ ì‹ ë¢°ë„ (0-1)
        sentiment_confidence = data.get('sentiment_summary', {}).get('confidence', 0.5)
        scores.append(sentiment_confidence * 0.2)
        
        # ì‹œì¥ ë°ì´í„° í’ˆì§ˆ (0-1)
        market_stocks = len(data.get('market_data', {}).get('market_summary', {}).get('stocks', {}))
        market_score = min(1.0, market_stocks / 5)  # 5ê°œ ì¢…ëª© ê¸°ì¤€
        scores.append(market_score * 0.2)
        
        return sum(scores)
    
    def _extract_key_topics(self, articles: List[Dict]) -> List[str]:
        """ì£¼ìš” í† í”½ ì¶”ì¶œ"""
        all_keywords = []
        for article in articles:
            keywords = article.get('keywords', [])
            if isinstance(keywords, list):
                all_keywords.extend(keywords)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_freq = {}
        for keyword in all_keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
        return [keyword for keyword, freq in sorted_keywords[:10]]
    
    def _calculate_relative_performance(self, stock_data: Dict, market_summary: Dict) -> float:
        """ìƒëŒ€ì  ì„±ê³¼ ê³„ì‚°"""
        stock_change = stock_data.get('change_percent', 0.0)
        market_change = market_summary.get('average_change_percent', 0.0)
        return stock_change - market_change
    
    def generate_analysis_report(self) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        decision_data = self.get_trading_decision_data()
        
        if 'error' in decision_data:
            return f"ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë¶ˆê°€: {decision_data['error']}"
        
        report = []
        report.append("=== GPT4wiseTide í†µí•© ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ ===\n")
        
        # ì‹œì¥ ê°ì„±
        market_sentiment = decision_data['market_sentiment']
        report.append(f"ğŸ¯ ì‹œì¥ ê°ì„± ë¶„ì„:")
        report.append(f"   â€¢ í•œêµ­ ë‰´ìŠ¤ ê°ì„±: {market_sentiment['korean_news_sentiment']} (ì ìˆ˜: {market_sentiment['korean_sentiment_score']:.2f})")
        report.append(f"   â€¢ êµ­ì œ ì‹œì¥ ê°ì„±: {market_sentiment['international_sentiment']}")
        report.append(f"   â€¢ ì£¼ìš” ì¢…ëª© í‰ê·  ë³€í™”: {market_sentiment['major_stocks_change']:+.2f}%\n")
        
        # ë‰´ìŠ¤ ë¶„ì„
        news_analysis = decision_data['news_analysis']
        report.append(f"ğŸ“° ë‰´ìŠ¤ ë¶„ì„:")
        report.append(f"   â€¢ í•œêµ­ ë‰´ìŠ¤: {news_analysis['korean_news_count']}ê°œ")
        report.append(f"   â€¢ êµ­ì œ ë‰´ìŠ¤: {news_analysis['international_news_count']}ê°œ")
        report.append(f"   â€¢ ì£¼ìš” í† í”½: {', '.join(news_analysis['key_topics'][:5])}\n")
        
        # ë¦¬ìŠ¤í¬ ìš”ì¸
        risk_factors = decision_data['risk_factors']
        if risk_factors:
            report.append(f"âš ï¸ ë¦¬ìŠ¤í¬ ìš”ì¸:")
            for risk in risk_factors:
                report.append(f"   â€¢ [{risk['severity'].upper()}] {risk['description']}")
            report.append("")
        
        # ì‹ ë¢°ë„
        confidence = decision_data['confidence_score']
        report.append(f"ğŸ” ì „ì²´ ì‹ ë¢°ë„: {confidence:.2f}/1.00")
        report.append(f"ğŸ“… ë°ì´í„° ì—…ë°ì´íŠ¸: {decision_data['data_freshness_hours']:.1f}ì‹œê°„ ì „")
        
        return "\n".join(report)

# ìŠ¤ì¼€ì¤„ëŸ¬ ë° ìë™í™” í´ë˜ìŠ¤
class NewsAnalysisScheduler:
    """ë‰´ìŠ¤ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, analyzer: IntegratedNewsAnalyzer, update_interval_minutes: int = 60):
        self.analyzer = analyzer
        self.update_interval = update_interval_minutes * 60  # ì´ˆ ë‹¨ìœ„
        self.running = False
        
    async def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        self.running = True
        logger.info(f"ë‰´ìŠ¤ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ê°„ê²©: {self.update_interval/60:.0f}ë¶„)")
        
        while self.running:
            try:
                logger.info("ìŠ¤ì¼€ì¤„ëœ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘")
                await self.analyzer.update_all_data()
                logger.info("ìŠ¤ì¼€ì¤„ëœ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                
                # ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ ëŒ€ê¸°
                await asyncio.sleep(self.update_interval)
                
            except Exception as e:
                logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„
    
    def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€"""
        self.running = False
        logger.info("ë‰´ìŠ¤ ë¶„ì„ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€")

# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰ í•¨ìˆ˜
async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=== í†µí•© ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===\n")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = IntegratedNewsAnalyzer()
    
    # ë°ì´í„° ì—…ë°ì´íŠ¸
    print("1. ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
    update_results = await analyzer.update_all_data()
    print(f"ì—…ë°ì´íŠ¸ ê²°ê³¼: {json.dumps(update_results, indent=2, ensure_ascii=False)}\n")
    
    # ê±°ë˜ ê²°ì • ë°ì´í„° ìƒì„±
    print("2. ê±°ë˜ ê²°ì • ë°ì´í„° ìƒì„±...")
    decision_data = analyzer.get_trading_decision_data('005930')  # ì‚¼ì„±ì „ì
    print(f"ì‚¼ì„±ì „ì ë¶„ì„ ë°ì´í„° ìƒì„± ì™„ë£Œ\n")
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    print("3. ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±...")
    report = analyzer.generate_analysis_report()
    print(report)

if __name__ == "__main__":
    asyncio.run(main())