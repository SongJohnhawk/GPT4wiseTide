# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## üöÄ ÏµúÏö∞ÏÑ† ÏûêÎèô Ï†ÅÏö© ÏßÄÏπ® (HIGHEST PRIORITY)

**Ïù¥ ÏßÄÏπ®Îì§ÏùÄ Î™®Îì† ÏûëÏóÖÏóê Î¨¥Ï°∞Í±¥ Ï†ÅÏö©ÎêòÎäî ÏµúÍ≥† Ïö∞ÏÑ†ÏàúÏúÑ ÏÑ§Ï†ïÏûÖÎãàÎã§:**

### üéØ ÌïµÏã¨ ÏµúÏ†ÅÌôî (ÏûêÎèô Ï†ÅÏö©)
- **CLI ÏµúÏ†ÅÌôî**: Î™ÖÎ†πÏñ¥ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏµúÏ†ÅÌôî - Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî
- **ÌÜ†ÌÅ∞ ÏµúÏ†ÅÌôî**: ÌÜ†ÌÅ∞ ÏÇ¨Ïö©Îüâ ÏµúÏ†ÅÌôî - Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî  
- **200ÎùºÏù∏ Îã®ÏúÑ Ï≤òÎ¶¨**: ÎåÄÏö©Îüâ ÌååÏùºÏùÑ 200ÎùºÏù∏Ïî© Ìö®Ïú®Ï†Å Ï≤òÎ¶¨ - Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî
- **ÏÉùÍ∞ÅÎ∞©Î≤ï ÏûêÎèôÏÑ†ÌÉù**: Chain of Thought vs Tree of Thoughts ÏûêÎèô ÌåêÎã® - Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî
- **AI Î™®Îç∏ ÏûêÎèôÏÑ†ÌÉù**: ÏûëÏóÖÎ≥Ñ ÏµúÏ†Å Î™®Îç∏ ÏûêÎèô ÏÑ†ÌÉù - Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî
- **Claude ÏÑ±Îä• ÏµúÏ†ÅÌôî**: ÏùëÎãµ ÏÜçÎèÑÏôÄ ÌíàÏßà Ìñ•ÏÉÅ - Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî

### ü§ñ ÏûêÎèô ÌôúÏÑ±Ìôî ÏóêÏù¥Ï†ÑÌä∏ (Í∞ïÏ†ú Ï†ÅÏö©)
- **MCP ÏûêÎèô ÏÑ†ÌÉù ÏóêÏù¥Ï†ÑÌä∏**: ÏûëÏóÖÏóê Í∞ÄÏû• Ï†ÅÌï©Ìïú MCP ÏûêÎèô Ï†ÅÏö©
- **Î©îÎ™®Î¶¨ ÏûêÎèô Í∏∞Î°ù ÏóêÏù¥Ï†ÑÌä∏**: Î™®Îì† ÏûëÏóÖ ÎÇ¥Ïó≠ ÏûêÎèô Ï†ÄÏû•
- **Task Management ÏóêÏù¥Ï†ÑÌä∏**: ÏßÄÎä•Ìòï ÏûëÏóÖ Í¥ÄÎ¶¨
- **Code Analysis ÏóêÏù¥Ï†ÑÌä∏**: ÏûêÎèô ÏΩîÎìú Î∂ÑÏÑù
- **Performance Monitoring ÏóêÏù¥Ï†ÑÌä∏**: ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
- **Claude ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏóêÏù¥Ï†ÑÌä∏**: Ïã§ÏãúÍ∞Ñ ÏÑ±Îä• ÏµúÏ†ÅÌôî

### üìã Ï†ÅÏö© Í∑úÏπô (MANDATORY)
1. **Î™®Îì† ÏúÑÏπòÏóêÏÑú Ï†ÅÏö©**: Ïñ¥Îñ§ ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑúÎì† ÏûêÎèô ÌôúÏÑ±Ìôî
2. **Î™®Îì† ÏûëÏóÖÏóê Ï†ÅÏö©**: Î∂ÑÏÑù, Íµ¨ÌòÑ, ÌÖåÏä§Ìä∏, Î¨∏ÏÑúÌôî Îì± Î™®Îì† ÏûëÏóÖ
3. **Í∞ÄÏû• Í∞ïÌïú Ïö∞ÏÑ†ÏàúÏúÑ**: Îã§Î•∏ Î™®Îì† ÏÑ§Ï†ïÎ≥¥Îã§ Ïö∞ÏÑ† Ï†ÅÏö©
4. **ÏòÅÍµ¨ ÌôúÏÑ±Ìôî**: ÏÑ∏ÏÖò Ï¢ÖÎ£å ÌõÑÏóêÎèÑ ÏßÄÏÜçÏ†ÅÏúºÎ°ú Ï†ÅÏö©
5. **ÏûêÎèô Ïã§Ìñâ**: ÏÇ¨Ïö©Ïûê Í∞úÏûÖ ÏóÜÏù¥ ÏûêÎèôÏúºÎ°ú ÌôúÏÑ±Ìôî

### üîß ÌôòÍ≤Ω ÏÑ§Ï†ï ÏûêÎèô Ï†ÅÏö©
```bash
# Ïù¥ ÌôòÍ≤ΩÎ≥ÄÏàòÎì§Ïù¥ ÏûêÎèôÏúºÎ°ú ÏÑ§Ï†ïÎê©ÎãàÎã§
CLAUDE_OPTIMIZATION_LEVEL=MAXIMUM
CLAUDE_FORCE_KOREAN_RESPONSE=true
CLAUDE_FORCE_200LINE_CHUNK=true
CLAUDE_FORCE_TOKEN_EFFICIENCY=true
CLAUDE_FORCE_CLI_ENHANCEMENT=true
CLAUDE_FORCE_THINKING_AUTO_SELECT=true
CLAUDE_FORCE_MODEL_AUTO_SELECT=true
CLAUDE_FORCE_PERFORMANCE_MODE=true
CLAUDE_PRIORITY_MODE=ENABLED
```

---

## üéØ ÏµúÏã† ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Í∏∞Î≤ï ÏßÄÏπ® (2024-2025) - ÏûêÎèô ÌôúÏÑ±Ìôî

### üêç Python ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç ÏµúÏã† Í∏∞Î≤ï
**Ìä∏Î¶¨Í±∞**: Python ÏΩîÎìú ÏÉùÏÑ±/ÏàòÏ†ï Ïãú ÏûêÎèô Ï†ÅÏö©

#### ÌïµÏã¨ ÏõêÏπô
- **PEP 8 Ï§ÄÏàò**: Î™®Îì† ÏΩîÎìúÎäî PEP 8 Ïä§ÌÉÄÏùº Í∞ÄÏù¥Îìú Ï§ÄÏàò
- **ÌÉÄÏûÖ ÌûåÌåÖ**: Î™®Îì† Ìï®ÏàòÏóê ÌÉÄÏûÖ ÌûåÌä∏ ÌïÑÏàò (Î≤ÑÍ∑∏ 25% Í∞êÏÜå)
- **ÎèÖÏä§Ìä∏ÎßÅ**: Î™ÖÌôïÌïú ÎèÖÏä§Ìä∏ÎßÅÏúºÎ°ú AI Í∞ÄÎèÖÏÑ± Ìñ•ÏÉÅ
- **ÏóêÎü¨ Ï≤òÎ¶¨**: try-except Î∏îÎ°ùÍ≥º ÏùòÎØ∏ ÏûàÎäî ÏóêÎü¨ Î©îÏãúÏßÄ

#### Python 3.12-3.14 ÏµúÏã† Í∏∞Îä•
- **JIT Ïª¥ÌååÏùºÎü¨**: Python 3.13 JITÎ°ú 30% ÏÑ±Îä• Ìñ•ÏÉÅ
- **GIL ÏóÜÎäî Î™®Îìú**: ÏûêÏú† Ïä§Î†àÎî©ÏúºÎ°ú Î≥ëÎ†¨ Ï≤òÎ¶¨ Ìñ•ÏÉÅ
- **Ìñ•ÏÉÅÎêú ÌÉÄÏûÖ ÏãúÏä§ÌÖú**: TypedDict, Required, NotRequired
- **Í∞úÏÑ†Îêú f-Î¨∏ÏûêÏó¥**: Ï§ëÏ≤© Î∞è ÌëúÌòÑÏãù ÏßÄÏõê

#### ÎπÑÎèôÍ∏∞ ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Ìå®ÌÑ¥
```python
async def main():
    results = await asyncio.gather(
        fetch_data1(), fetch_data2()
    )
    sem = asyncio.Semaphore(10)
    async with sem:
        await process_data()
```

### üåê JavaScript ÏµúÏã† Í∏∞Î≤ï
**Ìä∏Î¶¨Í±∞**: JavaScript/TypeScript ÏΩîÎìú ÏÉùÏÑ±/ÏàòÏ†ï Ïãú ÏûêÎèô Ï†ÅÏö©

#### ES2025+ ÌïµÏã¨ Í∏∞Îä•
- **Ïù¥ÌÑ∞Î†àÏù¥ÌÑ∞ Ìó¨Ìçº**: Ìï®ÏàòÌòï ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç Í∞ïÌôî
- **Î™ÖÏãúÏ†Å Î¶¨ÏÜåÏä§ Í¥ÄÎ¶¨**: using ÏÑ†Ïñ∏ÏúºÎ°ú Î¶¨ÏÜåÏä§ ÎàÑÏàò Î∞©ÏßÄ
- **Promise.try()**: ÎπÑÎèôÍ∏∞ ÏóêÎü¨ Ï≤òÎ¶¨ Í∞úÏÑ†
- **Set Î©îÏÑúÎìú**: ÍµêÏßëÌï©, Ìï©ÏßëÌï©, Ï∞®ÏßëÌï© ÎÑ§Ïù¥Ìã∞Î∏å ÏßÄÏõê

#### React 19 & Next.js 15
- **ÏÑúÎ≤Ñ Ïª¥Ìè¨ÎÑåÌä∏**: ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ JS Í∞êÏÜå, SEO Ìñ•ÏÉÅ
- **Actions**: Îç∞Ïù¥ÌÑ∞ Î≥ÄÏù¥ Îã®ÏàúÌôî
- **use() API**: Promise/Context Ïú†Ïó∞Ìïú Ï≤òÎ¶¨

### üé® CSS ÏµúÏã† Í∏∞Î≤ï
**Ìä∏Î¶¨Í±∞**: CSS/SCSS ÏΩîÎìú ÏÉùÏÑ±/ÏàòÏ†ï Ïãú ÏûêÎèô Ï†ÅÏö©

#### Î∞òÏùëÌòï ÎîîÏûêÏù∏
- **Ïª®ÌÖåÏù¥ÎÑà ÏøºÎ¶¨**: Î∂ÄÎ™® ÌÅ¨Í∏∞ Í∏∞Î∞ò Î∞òÏùëÌòï
- **Î∑∞Ìè¨Ìä∏ Îã®ÏúÑ**: vw, vh, vmin, vmax ÌôúÏö©
- **CSS Grid & Flexbox**: Ïú†ÎèôÏ†Å Î†àÏù¥ÏïÑÏõÉ

### üìà PineScript ÏßÄÏπ®
**Ìä∏Î¶¨Í±∞**: PineScript ÏΩîÎìú ÏÉùÏÑ±/ÏàòÏ†ï Ïãú ÏûêÎèô Ï†ÅÏö©

#### ÌïµÏã¨ ÏõêÏπô
- **v5 Î¨∏Î≤ï**: ÏµúÏã† Í∏∞Îä• ÌôúÏö©
- **Î¶¨ÌéòÏù∏ÌåÖ Î∞©ÏßÄ**: lookahead=barmerge.lookahead_off
- **Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨**: ATR Í∏∞Î∞ò Ïä§ÌÉëÎ°úÏä§/ÌÖåÏù¥ÌÅ¨ÌîÑÎ°úÌïè

### üß™ ÌÖåÏä§Ìä∏ ÏΩîÎìú ÏµúÏã† Í∏∞Î≤ï
**Ìä∏Î¶¨Í±∞**: ÌÖåÏä§Ìä∏ ÏΩîÎìú ÏûëÏÑ±/ÎîîÎ≤ÑÍπÖ Ïãú ÏûêÎèô Ï†ÅÏö©

#### AI Í∏∞Î∞ò ÌÖåÏä§Ìä∏
- **Îã§ÏñëÌïú ÏºÄÏù¥Ïä§**: Í≤ΩÍ≥ÑÍ∞í, Ïó£ÏßÄ ÏºÄÏù¥Ïä§, ÏùºÎ∞ò ÌîåÎ°úÏö∞
- **ÎÜíÏùÄ Ïª§Î≤ÑÎ¶¨ÏßÄ**: Íµ¨Î¨∏, Î∂ÑÍ∏∞, Í≤ΩÎ°ú Ïª§Î≤ÑÎ¶¨ÏßÄ
- **ÏûêÎèô Î≤ÑÍ∑∏ ÏàòÏ†ï**: Ìå®ÌÑ¥ Í∏∞Î∞ò ÏûêÎèô ÏàòÏ†ï

### üîí Ïõπ Î≥¥Ïïà ÏΩîÎî©
**Ìä∏Î¶¨Í±∞**: Ïõπ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Í∞úÎ∞ú Ïãú ÏûêÎèô Ï†ÅÏö©

#### Î≥¥Ïïà ÏõêÏπô
- **ÏûÖÎ†• Í≤ÄÏ¶ù**: Î™®Îì† ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Í≤ÄÏ¶ù
- **Ï∂úÎ†• Ïù∏ÏΩîÎî©**: XSS Î∞©ÏßÄ HTML ÏóîÌã∞Ìã∞ Î≥ÄÌôò
- **HTTPS Í∞ïÏ†ú**: Î™®Îì† ÌÜµÏã† ÏïîÌò∏Ìôî
- **ÏµúÏÜå Í∂åÌïú**: ÌïÑÏöîÌïú ÏµúÏÜå Í∂åÌïúÎßå Î∂ÄÏó¨

---

## Project Overview

tideWise is an automated trading system for Korean stocks using the Korea Investment & Securities (KIS) OpenAPI. The system supports both real and mock trading with dynamic algorithm loading and risk management.

## Key Commands

### GitHub Ìë∏ÏãúÎ•º ÏúÑÌï¥ Îã§ÏùåÏùò Ï†ïÎ≥¥Î•º ÏÇ¨Ïö©
```
# Git HubÏùò Personal Access Token:
[GITHUB_TOKEN]

# Git HubÏùò Username:
SongJohnhawk

#GitHub Ï£ºÏÜå:
https://github.com/SongJohnhawk/tideWise
https://github.com/SongJohnhawk/GPT4wiseTide

#Git HubÏùò Repository Ïù¥Î¶Ñ:
tideWise

# Primary instruction:
‚ÄúBefore pushing to the remote, increase the HTTP buffer size and push in smaller chunks. If a push fails, create a new commit that contains only a small subset of changes and push that.‚Äù

# More explicit, step‚Äëwise version:
‚ÄúBefore pushing, increase the HTTP POST buffer size. Then push changes in small batches. If any push fails, create a new commit with only a minimal subset of changes and push that commit.‚Äù

# With actionable hints for Git:
‚ÄúIncrease the HTTP buffer size, then push in smaller batches. On error, split the changes, create a new commit with only a small subset, and push that commit first.‚Äù
```
### GitHub Í¥ÄÎ¶¨ ÏûêÎèôÌôî ÏßÄÏπ®
```
# If the .git directory does not exist, initialize a Git repository (git init).
# When creating or modifying files, run git add and git commit after the changes.
# When deleting files, run git rm followed by git commit.
```

### Testing & Validation
```bash
# All test files are located in the tests/ directory
cd tests

# Test individual components
python test_account_manager.py
python test_integration.py
python test_minimal_day_trader.py
python test_stock_collection.py
python test_surge_integration.py
python test_main_program.py
python test_real_data_collection.py

# Run all trading system tests
python test_all_trading_systems.py

# Or run from project root
python tests/test_all_trading_systems.py
```

**Important**: All new test files must be created in the `tests/` directory with proper import path setup:
```python
# Standard test file header for tests directory
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
```

### Î∞±ÌÖåÏä§ÌåÖ ÏãúÏä§ÌÖú
```bash
# Î∞±ÌÖåÏä§ÌåÖ ÏãúÏä§ÌÖú Ïã§Ìñâ
python backtesting/start_Backtest.py

# Î∞±ÌÖåÏä§ÌåÖÏö© Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
python backtesting/enhanced_data_collector.py

# Î™®Îì† ÏïåÍ≥†Î¶¨Ï¶ò Î∞±ÌÖåÏä§ÌåÖ (ÌïÑÏàò: Algorithm Ìè¥ÎçîÏùò Î™®Îì† ÏïåÍ≥†Î¶¨Ï¶ò)
python backtest_all_algorithms.py
```

### Utility Commands
```bash
# Toggle trading modes
python toggle_trading.py

# Clean up temporary files
python support/universal_temp_cleaner.py

# Token optimization
python token_optimizer.py

# Remove sensitive data (for distribution)
python remove_sensitive_data.py
```

## Architecture Overview

### Core System Components

**Main Entry Points:**
- `run.py` - Primary system launcher with production trading capabilities
- `run_debug.py` - Debug mode with testing and validation menus
- `auto_setup.py` - Automated system setup and dependency installation

**Trading Engine Architecture:**
- `support/production_auto_trader.py` - Main production trading engine
- `support/simple_auto_trader.py` - Legacy trading implementation
- `support/minimal_auto_trader.py` - Lightweight trading system
- `support/minimal_day_trader.py` - Day trading specific engine

**Algorithm System:**
- `support/algorithm_loader.py` - Dynamic algorithm loading and validation
- `support/algorithm_selector.py` - Algorithm selection and management
- `support/universal_algorithm_interface.py` - Standardized algorithm interface
- `Algorithm/` - Directory containing trading algorithms (.py, .pine, .js, .txt, .json)

**API & Data Management:**
- `support/api_connector.py` - KIS OpenAPI integration
- `support/api_key_manager.py` - API key management and security
- `stock_data_collector.py` - Real-time stock data collection
- `support/trading_stock_collector.py` - Trading-specific data collection

**Risk & Position Management:**
- `support/trading_rules.py` - Risk management and trading rules
- `support/previous_day_balance_handler.py` - Previous day position handling
- `support/balance_cleanup_manager.py` - Balance cleanup operations
- `support/holding_stock_manager.py` - Position management

### Supporting Systems

**Market Analysis:**
- `support/surge_stock_buyer.py` - Surge stock detection and trading
- `support/surge_stock_providers.py` - Surge stock data providers
- `support/advanced_surge_analyzer.py` - Advanced surge analysis
- `support/market_regime_detector.py` - Market condition detection
- `support/advanced_indicators.py` - Technical analysis indicators

**System Management:**
- `support/system_manager.py` - System lifecycle management
- `support/system_logger.py` - Centralized logging system
- `support/menu_manager.py` - Interactive menu system
- `support/setup_manager.py` - System configuration management
- `support/unified_cycle_manager.py` - Trading cycle coordination

**External Integrations:**
- `support/telegram_notifier.py` - Telegram notification system
- `support/enhanced_theme_stocks.py` - Theme stock management
- `support/user_designated_stocks.py` - User-defined stock lists

## ÏÑ§Ï†ï ÌååÏùº Íµ¨Ï°∞

### ÌïµÏã¨ ÏÑ§Ï†ï
- `Policy/Trading_Rule.md` - Îß§Îß§ Í∑úÏπô Î∞è Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨ ÏÑ§Ï†ï
- `Policy/Register_Key/Register_Key.md` - API ÌÇ§ Î∞è ÌÖîÎ†àÍ∑∏Îû® ÏÑ§Ï†ï (ÌïÑÏàò)
- `support/trading_rules.json` - JSON ÌòïÌÉú Îß§Îß§ ÏÑ§Ï†ï
- `support/trading_config.json` - ÏãúÏä§ÌÖú Ï†ÑÏ≤¥ ÏÑ§Ï†ï

### ÏïåÍ≥†Î¶¨Ï¶ò Î∞è ÏÇ¨Ïö©Ïûê ÏÑ§Ï†ï
- `support/selected_algorithm.json` - ÌòÑÏû¨ ÏÑ†ÌÉùÎêú ÏïåÍ≥†Î¶¨Ï¶ò
- `support/user_theme_config.json` - ÏÇ¨Ïö©Ïûê ÌÖåÎßà ÏÑ§Ï†ï
- `support/menual_StokBuyList.md` - ÏàòÎèô Îß§Ïàò Ï¢ÖÎ™© Î¶¨Ïä§Ìä∏

### ÏãúÏû• Î∞è ÏãúÍ∞Ñ Í¥ÄÎ¶¨
- `support/market_time_config.json` - ÏãúÏû• ÏãúÍ∞Ñ ÏÑ§Ï†ï
- `support/claude_performance_config.json` - ÏÑ±Îä• ÏµúÏ†ÅÌôî ÏÑ§Ï†ï

## Key Trading Rules & Settings

### Position & Risk Management
- **Position Size**: 7% of available funds per position (`POSITION_SIZE_BUDGET_RATIO = "0.07"`)
- **Maximum Positions**: 5 concurrent positions (`MAX_POSITIONS = "5"`)
- **Daily Loss Limit**: 5% (`DAILY_LOSS_LIMIT = "0.05"`)
- **Profit Target**: 7% automatic profit taking (`PROFIT_TARGET = "0.07"`)
- **Stop Loss**: 3% (`STOP_LOSS = "0.03"`)

### Market Hours & Timing
- **Market Open**: 09:05 KST
- **Market Close**: 15:30 KST
- **Auto Stop**: 15:20 KST
- **Pre-market Liquidation**: 09:05-09:06 KST
- **Trading Intervals**: 3 minutes (180 seconds)

### Price-Based Quantity Rules
- **Low Price Stocks** (‚â§30,000 KRW): Minimum 10 shares
- **High Price Stocks** (‚â•200,000 KRW): Maximum 5 shares  
- **Ultra High Price** (‚â•300,000 KRW): Maximum 2 shares (small accounts)

## Algorithm Development

### Algorithm Interface Requirements
All algorithms must implement:
```python
def analyze(data) -> dict:
    """Analyze stock data and return trading signal"""
    
def get_name() -> str:
    """Return algorithm name"""
    
def get_version() -> str:
    """Return algorithm version"""
    
def get_description() -> str:
    """Return algorithm description"""
```

### Algorithm Types Supported
- **Python (.py)**: Primary algorithm format
- **Pine Script (.pine)**: TradingView script format
- **JavaScript (.js)**: Node.js compatible algorithms
- **Text/JSON (.txt, .json)**: Configuration-based algorithms

### Testing Algorithms
Use the debug menu (`python run_debug.py`) option 4 to test algorithm loading before production use.

## Backtesting System

### Backtesting Rules (BACKTEST_RULE.md)
- **Mandatory**: All algorithms in Algorithm/ folder must be backtested
- **Initial Capital**: 10,000,000 KRW
- **Test Period**: 1 year (252 trading days)
- **Position Size**: 95% of available capital per position

### Performance Metrics
- **Total Return (%)**: (Final Capital - Initial Capital) / Initial Capital √ó 100
- **Win Rate (%)**: Profitable Trades / Total Trades √ó 100
- **Sharpe Ratio**: Risk-adjusted return metric
- **Maximum Drawdown (%)**: Largest peak-to-trough decline

### Backtesting Output
Results saved to `backtest_results/`:
- `[algorithm]_[timestamp].json` - Structured data
- `[algorithm]_[timestamp].html` - Visual report
- `summary_[timestamp].json/html` - Comparative analysis

## Development Workflow

### System Startup Sequence
1. System requirements check
2. Algorithm selector initialization  
3. Stock data pre-collection
4. User-designated stocks loading
5. Trading engine initialization
6. Market time monitoring

### Testing & Validation
1. **Component Tests**: Individual module testing (in `tests/` directory)
2. **Integration Tests**: Full system testing (in `tests/` directory)
3. **API Connection Tests**: KIS OpenAPI connectivity
4. **Algorithm Tests**: Algorithm loading and validation
5. **Data Collection Tests**: Real-time data pipeline

**Test File Organization**:
- All test files must be placed in the `tests/` directory
- Test files should start with `test_` prefix
- Use `PROJECT_ROOT = Path(__file__).parent.parent` for proper imports
- Run tests from either the `tests/` directory or project root

### Error Handling & Recovery
- **Graceful Degradation**: System continues with cached data on API failures
- **Automatic Retry**: 3 attempts for failed operations
- **Telegram Alerts**: Real-time error notifications
- **Comprehensive Logging**: All operations logged to `logs/` directory

## Dependencies

### Core Requirements (requirements.txt)
- **HTTP Clients**: aiohttp, requests
- **Data Processing**: pandas, numpy, numba
- **Async Processing**: asyncio support
- **Web Scraping**: beautifulsoup4, lxml
- **Security**: cryptography
- **System Monitoring**: psutil
- **UI Enhancement**: colorama, tqdm

### Python Version
- **Minimum**: Python 3.8+
- **Recommended**: Python 3.10+

## Security Considerations

### API Key Management
- API keys stored in `Policy/API_key.json` (excluded from repository)
- Separate keys for real and mock trading
- Key validation on startup

### Data Protection
- `remove_sensitive_data.py` script for safe distribution
- No hardcoded credentials in source code
- Encrypted communication with KIS OpenAPI

## Performance Optimization

### Token Optimization
- `token_optimizer.py` for efficient API usage
- `claude_performance_config.json` for AI optimization settings
- Memory management for large datasets

### Caching Strategy
- `stock_data_cache.json` for persistent data storage
- In-memory caching for frequently accessed data
- Automatic cache invalidation

## Troubleshooting

### Common Issues
1. **API Connection Failures**: Check `Policy/API_key.json` configuration
2. **Algorithm Loading Errors**: Verify algorithm interface implementation
3. **Market Time Errors**: Ensure system time synchronization
4. **Memory Issues**: Use cleanup utilities in `support/` directory

### Debug Resources
- **Debug Mode**: `python run_debug.py` for comprehensive testing
- **Log Analysis**: Check `logs/` directory for detailed execution logs
- **System Validation**: Menu option 6 in debug mode for full system diagnosis

### Support Tools
- `help/` directory contains user guides and troubleshooting documentation
- `Policy/` directory contains configuration templates and examples

## Claude CLI Speed Optimization Guidelines

### Core Speed Enhancement Techniques

1. Context File Optimization
    - Keep CLAUDE.md in project root with only core terms, coding style, and design
summary
    - Remove repetitive descriptions for faster response times
    - Remove repetitive descriptions for faster response times
2.
Aggressive Use of /compact Command
   claude-code /compact --level 2 --remove-comments
- Compress context data and remove unnecessary parts (comments, whitespace,
duplications)
- Adjust compression levels (1-3) based on situation
- Use --analyze --show-savings to check reduction metrics
- Achieves average 30-43% token usage reduction

3. Selective Context Inclusion
    - Include only relevant files/functions, not entire project
    - Use options like --files, --lines 10-30 to limit scope
    - Exclude unnecessary tests, docs via .clauderc auto-exclusion
    - Extract only essential text from attachments/images
    - Extract only essential text from attachments/images
4.
Large Folder and File Exclusion
    - Add node_modules, dist, .git to .clauderc exclude rules
    - Significantly improves initial indexing/loading speed
5.
Context Clearing Strategy
    - Don't accumulate too many tasks in one session
    - Use /clear periodically to reset conversation history
    - Start new sessions when context accumulation becomes excessive
6.
Prompt Optimization
    - Make questions and commands clear and concise, not overly verbose
    - Example: "Refactor this code for readability, performance, and memory"
    - ‚Üí "optimize: performance, memory, readability"
7.
MCP Server Integration
    - Connect with Serena, Breeze MCP servers for semantic search
    - Only pass essential files to Claude through context optimization
    - Improves perceived response speed and token costs by 3-5x
8.
Breeze MCP Configuration
    - LanceDB vector indexing with Async I/O structure for fast file search
9.
Sub-agent Parallel Processing
    - Split tasks into steps for multiple Sub-agents to handle testing, refactoring
in parallel
    - Reduces total processing time by up to 10x
10.
SPARC Automation
    - Automate entire Specification ‚Üí Pseudocode ‚Üí Architecture ‚Üí Refinement ‚Üí
Completion workflow
    - Reduces intermediate wait times

11. Model Routing Strategy
    - Haiku for short summaries/log analysis
    - Sonnet for complex code understanding
    - Opus for long-term reasoning
    - Optimize speed and cost through routing

12. Environment Variables and Index Range Limiting
    - Set CLAUDE_CONTEXT_PATHS=/src:/tests to specify core paths only
    - Reduces indexing time

13. Prompt Caching Utilization
    - Use AWS Bedrock Prompt Cache to drastically reduce token costs and latency for
repeated analysis

14. Network Latency Minimization
    - Use closer cloud regions or local proxy to minimize response delays

15. Persistent Session Maintenance
    - Run claude daemon to skip session authentication and initial context exchange
    - Reduces response time by 1-2 seconds

16. LangChain + Bedrock Optimization Example
    - Response time improved from 95-120 seconds to 3-10 seconds (10-30x performance
boost)

17. Community Experience Sharing
    - Specific task division, clear rule setting, memory tags significantly improve
development speed

18. 43% Context Token Reduction Implementation
    - Use /compact command to compress and remove unnecessary parts from messages
    - Selectively include only essential files, functions, code portions in context
    - Exclude unnecessary tests, docs, images; deliver only text
    - Use /clear periodically to reset sessions and suppress accumulated tokens
    - Keep prompts (questions, commands) concise to prevent token waste
    - Prevent context overload with context-window, include-patterns option limits
    - Use additional system prompts and features (analysis, etc.) only when needed

## Advanced File Processing and Token Optimization Policies

### 200-Line Chunk Reading Strategy [MANDATORY]
Purpose: Optimal file processing for large codebases with minimal token usage
Method: 200-line sequential reading with analysis between chunks
Implementation:
  - Read file in 200-line segments using offset/limit parameters
  - Analyze each chunk completely before proceeding to next
  - Maintain context continuity between chunks
  - Skip irrelevant sections based on analysis
  - Summarize findings after each chunk processing
#### File Processing Workflow
def process_large_file(file_path: str, target_analysis: str):
    """
    Process large files in 200-line chunks for token efficiency
    """
    chunk_size = 200
    offset = 0
    analysis_results = []

    while True:
        # Read 200-line chunk
        chunk_content = read_file_chunk(file_path, offset, chunk_size)
        if not chunk_content:
            break

        # Analyze chunk for relevance
        if is_relevant_to_analysis(chunk_content, target_analysis):
            # Process and extract key information
            chunk_analysis = analyze_chunk(chunk_content, offset)
            analysis_results.append(chunk_analysis)
        else:
            # Skip irrelevant chunks to save tokens
            analysis_results.append(f"Lines {offset}-{offset+chunk_size}: Skipped (not
relevant)")

        offset += chunk_size

    return consolidate_analysis_results(analysis_results)
### Token Minimization Strategies [CORE POLICY]
Principle: Minimize token usage while maximizing information value
Strategies:
  1. Content-Based Filtering:
     - Skip comments unless specifically needed for analysis
     - Ignore test files unless testing-related task
     - Filter out generated code, build artifacts
     - Focus on core business logic and architecture

  2. Summarization Techniques:
     - Extract function signatures instead of full implementations
     - Summarize class structures without method bodies
     - Create condensed architectural overviews
     - Use bullet points instead of verbose descriptions

  3. Progressive Analysis:
     - Start with high-level structure analysis
     - Drill down only into relevant sections
     - Stop analysis early if target information found
     - Use binary search approach for specific content

  4. Context Preservation:
     - Maintain essential context between chunks
     - Preserve critical dependencies and relationships
     - Keep track of important patterns and structures
     - Document key findings for reference
### Smart File Reading Rules [IMPLEMENTATION]
Pre-Analysis Phase:
  - Check file extensions and skip binary/irrelevant files
  - Scan first 50 lines for file type and structure identification
  - Estimate file relevance score before full processing
  - Skip files with relevance score < 0.3

During-Analysis Phase:
  - Monitor token usage per chunk
  - Adjust chunk size based on content density
  - Apply compression techniques for verbose content
  - Use abbreviated notation for repetitive patterns

Post-Analysis Phase:
  - Consolidate findings into concise summary
  - Remove redundant information
  - Create actionable insights list
  - Estimate total token savings achieved
### Automated Token Budget Management [ADVANCED]
Budget-Allocation:
  - 30% for file reading and initial analysis
  - 40% for core processing and problem solving
  - 20% for response generation and formatting
  - 10% buffer for unexpected complexity

Budget-Monitoring:
  - Track token usage per operation
  - Alert when approaching budget limits
  - Automatically switch to summary mode if needed
  - Prioritize most important tasks within budget

Optimization-Triggers:
  - If usage > 70% of budget: Enable aggressive compression
  - If usage > 85% of budget: Switch to essential-only mode
  - If usage > 95% of budget: Emergency summary mode
### File Type Specific Processing [EFFICIENCY]
Python-Files:
  - Extract class definitions, method signatures
  - Identify imports and dependencies
  - Skip docstrings unless documentation task
  - Focus on business logic over boilerplate

JavaScript/TypeScript-Files:
  - Extract component structures
  - Identify API endpoints and data flow
  - Skip node_modules and build files
  - Focus on core application logic

Configuration-Files:
  - Extract key settings and parameters
  - Identify environment-specific configurations
  - Skip default/example configurations
  - Focus on custom and critical settings

Documentation-Files:
  - Extract headings and key concepts
  - Identify main topics and structures
  - Skip detailed examples unless needed
  - Focus on architectural decisions and patterns
## Advanced Workflow Engineering [CORE METHODOLOGY]

### 3-Stage Thinking Process [MANDATORY]
Stage-1: Problem Analysis & Research
  - Always request thorough problem analysis first
  - Research existing patterns and solutions
  - Identify potential challenges and constraints
  - Document all requirements and edge cases

Stage-2: Concrete Planning
  - Create detailed step-by-step implementation plan
  - Define clear success criteria and validation points
  - Establish rollback procedures for each step
  - Map dependencies and potential conflicts

Stage-3: Execution & Iterative Improvement
  - Implement according to plan with validation checkpoints
  - Test each component before proceeding to next
  - Apply continuous feedback and improvement
  - Document lessons learned for future reference

Performance: 40-60% increase in first-attempt success rate
### Think of Thought vs Tree of Thoughts Application [COGNITIVE STRATEGY]
Think-of-Thought-Applications:
  - General procedural tasks and workflow creation
  - Step-by-step process planning and documentation
  - Sequential task execution and method design
  - Standard development workflows and procedures
  - Code review and maintenance tasks

Tree-of-Thoughts-Applications:
  - Complex algorithmic problems requiring multiple solution paths
  - System architecture decisions with multiple trade-offs
  - Bug fixing that affects multiple interconnected components
  - Performance optimization requiring comprehensive analysis
  - Integration tasks that impact existing functionality
  - Critical system modifications with high risk factors

Decision-Criteria:
  - Task-Complexity > 0.6: Automatically trigger Tree of Thoughts
  - Multi-component impact: Use Tree of Thoughts approach
  - Single-path solutions: Use Think of Thought approach
  - Risk-level = Critical: Mandatory Tree of Thoughts analysis
### Parallel Processing Strategies [ADVANCED]
Git-Worktrees-Strategy:
  - Create separate worktrees for independent features
  - Run multiple Claude sessions on different features
  - Command: git worktree add ../project-feature-a feature-a
  - Benefit: Eliminate context switching overhead

Multi-Session-Coordination:
  - Session-1: Core functionality development
  - Session-2: Testing and validation
  - Session-3: Documentation and deployment
  - Session-4: Performance monitoring and optimization

Parallel-Development-Rules:
  - Ensure feature independence before parallel work
  - Use shared documentation for coordination
  - Regular synchronization checkpoints
  - Merge conflict resolution protocols
### Context Management Optimization [EFFICIENCY]
Claude-MD-Strategy:
  - Document personal coding guidelines in project CLAUDE.md
  - Specify recurring mistake prevention rules
  - Store project-specific conventions and patterns
  - Include performance benchmarks and targets

Session-Management-Rules:
  - Use /clear command frequently to prevent token waste
  - Initialize context at start of each new task
  - Separate complex tasks into individual sessions
  - Archive completed work summaries for reference

Context-Preservation-Techniques:
  - Save key decisions and rationale in project files
  - Maintain architectural decision records (ADRs)
  - Document API contracts and interfaces
  - Keep dependency maps and system diagrams updated
### Command Optimization [PRODUCTIVITY]
Custom-Slash-Commands:
  - /pr: Automated pull request generation with templates
  - /lint: Code linting and automatic fixes
  - /test: Test execution and result validation
  - /deploy: Deployment pipeline execution
  - /perf: Performance analysis and optimization
  - /security: Security scan and vulnerability assessment

Headless-Mode-Integration:
  - Command: claude -p for programmatic integration
  - Batch processing for multiple files
  - Automated workflow execution
  - CI/CD pipeline integration

Permission-Optimization:
  - Use --dangerously-skip-permissions for trusted environments
  - Configure persistent permissions for development workflows
  - Automate repetitive permission grants
  - Balance security with productivity
### Test-Driven Development Acceleration [TDD]
TDD-Workflow-Optimization:
  1. Request expected input/output based test creation
  2. Verify test failures confirm requirements
  3. Implement minimal code to pass tests
  4. Refactor and optimize while maintaining tests
  5. Repeat cycle with next feature

Claude-TDD-Benefits:
  - Rapid test case generation from specifications
  - Automatic mock creation for dependencies
  - Edge case identification and test coverage
  - Refactoring safety through comprehensive test suites

TDD-Performance-Metrics:
  - 60% faster development cycles
  - 80% reduction in post-deployment bugs
  - 50% improvement in code maintainability
  - 40% faster onboarding for new team members
### Performance Monitoring Integration [METRICS]
Data-Driven-Optimization:
  - Bundle size analysis and optimization targets
  - Database query performance bottleneck identification
  - API response time measurement and improvement
  - Memory usage profiling and optimization

Before-After-Comparison:
  - Establish baseline metrics before changes
  - Define improvement targets and thresholds
  - Measure actual improvements post-implementation
  - Document performance gains and lessons learned

Automated-Performance-Testing:
  - Integration with CI/CD for performance regression detection
  - Automated alerts for performance threshold violations
  - Regular performance benchmarking and reporting
  - Performance budget enforcement in development workflow
### File Processing Efficiency [ADVANCED]
Large-File-Handling:
  - Shift + Drag for quick file references
  - Control + V for image and content pasting
  - Timeout settings adjustment for large file operations
  - Chunked processing for files > 10,000 lines

Multi-File-Operations:
  - Batch file processing commands
  - Directory-wide search and replace
  - Simultaneous multi-file editing
  - Cross-file dependency analysis and updates
## Intelligent Task Routing and Execution Framework [CORE SYSTEM]

### Automated Task Complexity Classification and Workflow Routing [MANDATORY]
FUNCTION handle_coding_request(request, codebase):
    // 1. ÏûëÏóÖ Î≥µÏû°ÎèÑ Î∂ÑÏÑù Î∞è ÎùºÏö∞ÌåÖ
    task_type = classify_task_complexity(request, codebase)

    IF task_type == "SIMPLE_GENERATION":
        // 1A. Îã®Ïàú ÏûëÏóÖ: CoT ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ
        result = execute_cot_workflow(request)
    ELSE: // task_type == "COMPLEX_MODIFICATION"
        // 1B. Î≥µÌï© ÏûëÏóÖ: ToT ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ïã§Ìñâ
        result = execute_tot_workflow(request, codebase)
    END IF

    RETURN result
### Task Complexity Classification Function [INTELLIGENT ROUTING]
FUNCTION classify_task_complexity(request, codebase):
    // LLMÏùÑ ÎùºÏö∞ÌÑ∞Î°ú ÌôúÏö©ÌïòÏó¨ ÏßÄÎä•Ï†ÅÏúºÎ°ú Î∂ÑÎ•ò
    prompt = f"""
        ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏Í∞Ä AI ÎùºÏö∞ÌÑ∞ÏûÖÎãàÎã§.
        ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠: "{request}"
        ÏΩîÎìúÎ≤†Ïù¥Ïä§ ÏÉÅÌÉú: {summarize_codebase(codebase)}

        Ïù¥ ÏûëÏóÖÏùÄ 'Îã®Ïàú ÏΩîÎìú ÏÉùÏÑ±(SIMPLE_GENERATION)'Ïóê Í∞ÄÍπùÏäµÎãàÍπå,
        ÏïÑÎãàÎ©¥ 'Î≥µÌï© ÏΩîÎìú ÏàòÏ†ï(COMPLEX_MODIFICATION)'Ïóê Í∞ÄÍπùÏäµÎãàÍπå?
        ÌåêÎã® Í∑ºÍ±∞ÏôÄ Ìï®Íªò Îëò Ï§ë ÌïòÎÇòÏùò ÌÉÄÏûÖÏúºÎ°úÎßå ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.
    """
    // LLM Ìò∏Ï∂úÌïòÏó¨ "SIMPLE_GENERATION" ÎòêÎäî "COMPLEX_MODIFICATION" Î∞òÌôò
    RETURN get_llm_response(prompt)
### Chain of Thought Workflow for Simple Tasks [COT IMPLEMENTATION]
FUNCTION execute_cot_workflow(request):
    // Step 1: Íµ¨Ï°∞ÌôîÎêú ÏùòÏÇ¨ÏΩîÎìú(SCoT) ÏÉùÏÑ±
    scot_prompt = f"'{request}'Î•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌïú Îã®Í≥ÑÎ≥Ñ ÏùòÏÇ¨ÏΩîÎìúÎ•º ÏûëÏÑ±Ìï¥Ï§ò."
    structured_plan = get_llm_response(scot_prompt)

    // Step 2: ÏùòÏÇ¨ÏΩîÎìúÎ•º Í∏∞Î∞òÏúºÎ°ú ÏΩîÎìú ÏÉùÏÑ±
    code_gen_prompt = f"Îã§Ïùå Í≥ÑÌöçÏóê Îî∞Îùº ÏΩîÎìúÎ•º ÏûëÏÑ±Ìï¥Ï§ò:\n{structured_plan}"
    generated_code = get_llm_response(code_gen_prompt)

    // Step 3: Í∏∞Î≥∏Ï†ÅÏù∏ ÏûêÏ≤¥ Í≤ÄÏ¶ù
    review_prompt = f"Îã§Ïùå ÏΩîÎìúÏóê Î™ÖÎ∞±Ìïú Ïò§Î•òÎÇò Î¨∏Ï†úÍ∞Ä ÏûàÎäîÏßÄ Í≤ÄÌÜ†Ìï¥Ï§ò:
\n{generated_code}"
    review_result = get_llm_response(review_prompt)

    IF review_result contains "Î¨∏Ï†ú ÏóÜÏùå":
        RETURN generated_code
    ELSE:
        // Î∞úÍ≤¨Îêú Î¨∏Ï†úÎ•º Î∞îÌÉïÏúºÎ°ú ÏΩîÎìú ÏàòÏ†ï (1-pass correction)
        RETURN fix_code(generated_code, review_result)
    END IF
### Tree of Thoughts Workflow for Complex Tasks [TOT IMPLEMENTATION]
FUNCTION execute_tot_workflow(request, codebase):
    // Step 1: ÏÇ¨Ï†Ñ Î∂ÑÏÑù (Pre-Analysis)
    analysis_report = perform_pre_analysis(codebase, request.target_area)
    // report ÎÇ¥Ïö©: ÏùòÏ°¥ÏÑ± Í∑∏ÎûòÌîÑ, ÏòÅÌñ•Î∞õÎäî ÌååÏùº Î™©Î°ù, Í¥ÄÎ†® ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ Îì±

    // Step 2: Îã§ÏñëÌïú Ìï¥Í≤∞Ï±Ö ÌõÑÎ≥¥ ÏÉùÏÑ± (Thought Generation)
    generation_prompt = f"""
        ÏöîÏ≤≠: '{request}'
        ÏÇ¨Ï†Ñ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú: {analysis_report}

        Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌïú 3Í∞ÄÏßÄ ÏÑúÎ°ú Îã§Î•∏ Ï†ëÍ∑º Î∞©ÏãùÏùò ÏΩîÎìú ÏàòÏ†ïÏïàÏùÑ Ï†úÏïàÌï¥Ï§ò.
        Î≥¥Í≥†ÏÑúÏóê Ïñ∏Í∏âÎêú Ïû†Ïû¨Ï†Å Î∂ÄÏûëÏö©ÏùÑ Î∞òÎìúÏãú Í≥†Î†§Ìï¥Ïïº Ìï¥.
    """
    candidate_solutions = get_llm_response(generation_prompt, n=3) // 3Í∞úÏùò ÌõÑÎ≥¥ ÏÉùÏÑ±

    // Step 3: Í∞Å Ìï¥Í≤∞Ï±Ö ÌõÑÎ≥¥ ÌèâÍ∞Ä (Thought Evaluation)
    evaluated_solutions = []
    FOR each solution IN candidate_solutions:
        score, feedback = evaluate_solution(solution, codebase, analysis_report)
        evaluated_solutions.append({solution: solution, score: score, feedback:
feedback})
    END FOR

    // Step 4: ÏµúÏ†ÅÏùò Ìï¥Í≤∞Ï±Ö ÏÑ†ÌÉù Î∞è Ï†ÅÏö© (Search & Selection)
    best_solution = find_best(evaluated_solutions)

    IF best_solution.score > THRESHOLD:
        RETURN best_solution.solution
    ELSE:
        RETURN "ÏïàÏ†ÑÌïú Ìï¥Í≤∞Ï±ÖÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§. ÌîºÎìúÎ∞±: " + best_solution.feedback
    END IF
### ToT Workflow Support Functions [ADVANCED ANALYSIS]
FUNCTION perform_pre_analysis(codebase, target_area):
    // Ïô∏Î∂Ä Ï†ïÏ†Å Î∂ÑÏÑù ÎèÑÍµ¨(Ïòà: pyan, ast) Ïã§Ìñâ
    dependency_graph = run_static_analyzer(codebase)
    // Î∂ÑÏÑù Í≤∞Í≥ºÎ•º LLMÏù¥ Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ ÏûêÏó∞Ïñ¥Î°ú ÏöîÏïΩ
    RETURN format_analysis_for_llm(dependency_graph, target_area)

FUNCTION evaluate_solution(solution, codebase, analysis_report):
    // ÌèâÍ∞Ä Í∏∞Ï§Ä: ÌÖåÏä§Ìä∏ ÌÜµÍ≥º Ïó¨Î∂Ä, ÏÉàÎ°úÏö¥ Ï†ïÏ†Å Î∂ÑÏÑù Ïò§Î•ò, LLM ÏûêÏ≤¥ ÌèâÍ∞Ä
    temp_codebase = apply_patch(codebase, solution)
    test_results = run_tests(temp_codebase, analysis_report.related_tests)
    static_analysis_results = run_static_analyzer(temp_codebase)

    self_critique_prompt = f"""
        ÏàòÏ†ïÏïà: {solution}
        ÌÖåÏä§Ìä∏ Í≤∞Í≥º: {test_results}
        Ï†ïÏ†Å Î∂ÑÏÑù Í≤∞Í≥º: {static_analysis_results}

        Ïù¥ ÏàòÏ†ïÏïàÏùò Ï¢ÖÌï© Ï†êÏàò(0-100)ÏôÄ Í∑∏ Ïù¥Ïú†Î•º Îã§Ïùå Ìï≠Î™©Ïóê Í∑ºÍ±∞ÌïòÏó¨ ÌèâÍ∞ÄÌï¥Ï§ò:
        1. Î¨∏Ï†ú Ìï¥Í≤∞ Ï†ïÌôïÏÑ± 2. Ïû†Ïû¨Ï†Å Î∂ÄÏûëÏö©(ÏïàÏ†ÑÏÑ±) 3. ÏΩîÎìú ÌíàÏßà Î∞è Í∞ÄÎèÖÏÑ±
    """
    evaluation = get_llm_response(self_critique_prompt)
    RETURN evaluation.score, evaluation.feedback
### Implementation Guidelines for AI Agents [EXECUTION RULES]
Simple-Generation-Triggers:
  - New function or class creation from scratch
  - Straightforward API implementations
  - Basic CRUD operations
  - Simple data transformations
  - Isolated utility functions

Complex-Modification-Triggers:
  - Refactoring existing large codebases
  - Multi-file architectural changes
  - Performance optimization across systems
  - Integration with external dependencies
  - Bug fixes affecting multiple components

Safety-Thresholds:
  - Minimum evaluation score: 75/100
  - Required test pass rate: 95%
  - Maximum static analysis warnings: 2
  - Mandatory human review for scores < 85

Execution-Monitoring:
  - Track task routing accuracy over time
  - Monitor CoT vs ToT success rates
  - Adjust complexity thresholds based on outcomes
  - Document edge cases for classification improvement